{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import required library packages",
   "id": "e4ad365e16f3e9ff"
  },
  {
   "cell_type": "code",
   "id": "c6190eb6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, roc_auc_score, roc_curve,ConfusionMatrixDisplay, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T21:40:53.423688Z",
     "start_time": "2024-12-20T21:40:49.509357Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --upgrade scikit-learn",
   "id": "67ba671b37602729",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\chine\\appdata\\roaming\\python\\python310\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\chine\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\chine\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chine\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chine\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import dataset",
   "id": "1b6002355249931a"
  },
  {
   "cell_type": "code",
   "id": "fb0b2fd7",
   "metadata": {},
   "source": [
    "# copy cvs data to memory\n",
    "loan_df = pd.read_csv(\"Loan Prediction Dataset.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "736a861a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# see the first 5 data row\n",
    "loan_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f12c2c4",
   "metadata": {},
   "source": [
    "# dimension of data set\n",
    "loan_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63c9a967",
   "metadata": {},
   "source": [
    "# data types of columns in data set\n",
    "loan_df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7012dc05",
   "metadata": {},
   "source": [
    "### Analyse and fill in missing data"
   ]
  },
  {
   "cell_type": "code",
   "id": "086aa583",
   "metadata": {},
   "source": [
    "# sum up missing data per column\n",
    "loan_df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6d857fa",
   "metadata": {},
   "source": [
    "#### Fill in categorical missing features with the mode"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e616bd6",
   "metadata": {},
   "source": [
    "# create separate memory for dataset with no missing data\n",
    "loan_no_missing_data_df = loan_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d3c04c6",
   "metadata": {},
   "source": [
    "# Gender, Married, Dependents, Self_Employed, Loan_Amount_Term and Credit_History features are categorised, \n",
    "# therefore, we use the modes to fill in missing datas\n",
    "\n",
    "# Gender\n",
    "loan_no_missing_data_df.Gender.fillna(loan_no_missing_data_df.Gender.mode()[0], inplace=True)\n",
    "# Married\n",
    "loan_no_missing_data_df.Married.fillna(loan_no_missing_data_df.Married.mode()[0], inplace=True)\n",
    "# Dependents\n",
    "loan_no_missing_data_df.Dependents.fillna(loan_no_missing_data_df.Dependents.mode()[0], inplace=True)\n",
    "# Self_Employed\n",
    "loan_no_missing_data_df.Self_Employed.fillna(loan_no_missing_data_df.Self_Employed.mode()[0], inplace=True)\n",
    "# Loan_Amount_Term\n",
    "loan_no_missing_data_df.Loan_Amount_Term.fillna(loan_no_missing_data_df.Loan_Amount_Term.mode()[0], inplace=True)\n",
    "# Credit_History\n",
    "loan_no_missing_data_df.Credit_History.fillna(loan_no_missing_data_df.Credit_History.mode()[0], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b87a77a5",
   "metadata": {},
   "source": [
    "#### Fill in continuous missing features with the mean"
   ]
  },
  {
   "cell_type": "code",
   "id": "32c3f371",
   "metadata": {},
   "source": [
    "# LoanAmount feature is continuous,therefore, we use the mean to fill in missing datas\n",
    "loan_no_missing_data_df.LoanAmount.fillna(loan_no_missing_data_df.LoanAmount.mean(), inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc77e4bb",
   "metadata": {},
   "source": [
    "# confirm there is no more missing data\n",
    "loan_no_missing_data_df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "741f6f0c",
   "metadata": {},
   "source": [
    "# save your new dataset with no missing data\n",
    "loan_no_missing_data_df.to_csv(\"loan-no-missing-data.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "866b395a",
   "metadata": {},
   "source": [
    "# create a separate DataFrame for filling in your missing data\n",
    "loan_no_missing_data_df = pd.read_csv(\"loan-no-missing-data.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec56fd4c",
   "metadata": {},
   "source": [
    "loan_no_missing_data_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d4f9b4f",
   "metadata": {},
   "source": [
    "## Check for outliers in continuous features"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ae49ce2",
   "metadata": {},
   "source": [
    "loan_no_missing_data_describe_df = loan_no_missing_data_df[[\"ApplicantIncome\",\"CoapplicantIncome\", \"LoanAmount\"]].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35e310de",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "loan_no_missing_data_describe_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5037b472",
   "metadata": {},
   "source": [
    "# InterQuartile range check, to find lower and higher outliers per continuous features\n",
    "def IQR_check(df):\n",
    "    IQR_info = {}\n",
    "    for col in df.columns:\n",
    "        # dictionary of lower and higher outliers per numeric features, using Q1 - 1.5*IQR & Q3 - 1.5*IQR \n",
    "        IQR_info[col] = [df[col].loc[\"25%\"] - 1.5*df[col].loc[\"25%\"], df[col].loc[\"75%\"] + 1.5*df[col].loc[\"75%\"]]\n",
    "    return IQR_info"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f2a4513",
   "metadata": {},
   "source": [
    "# call the IQR function with data\n",
    "IQR_check(loan_no_missing_data_describe_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9397d9b",
   "metadata": {},
   "source": [
    "IQR_range = IQR_check(loan_no_missing_data_describe_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "649911dc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a column to indicate if `ApplicantIncome` column is an outlier using the lower and higher limit\n",
    "loan_no_missing_data_df[\"ApplicantIncome_is_outlier\"] = loan_no_missing_data_df[\"ApplicantIncome\"].apply(lambda x: \"Yes\" if x < IQR_range[\"ApplicantIncome\"][0] or x > IQR_range[\"ApplicantIncome\"][1] else \"No\")\n",
    "\n",
    "# Create a column to indicate if `CoapplicantIncome` column is an outlier using the lower and higher limit\n",
    "loan_no_missing_data_df[\"CoapplicantIncome_is_outlier\"] = loan_no_missing_data_df[\"CoapplicantIncome\"].apply(lambda x: \"Yes\" if x < IQR_range[\"CoapplicantIncome\"][0] or x > IQR_range[\"CoapplicantIncome\"][1] else \"No\")\n",
    "\n",
    "# Create a column to indicate if `LoanAmount` column is an outlier using the lower and higher limit\n",
    "loan_no_missing_data_df[\"LoanAmount_is_outlier\"] = loan_no_missing_data_df[\"LoanAmount\"].apply(lambda x: \"Yes\" if x < IQR_range[\"LoanAmount\"][0] or x > IQR_range[\"LoanAmount\"][1] else \"No\")\n",
    "\n",
    "# Create a column to indicate if all of `ApplicantIncome, CoapplicantIncome or LoanAmount` columns is an outlier\n",
    "loan_no_missing_data_df['All_outliers'] = loan_no_missing_data_df[[\"ApplicantIncome_is_outlier\", \"CoapplicantIncome_is_outlier\", \"LoanAmount_is_outlier\"]].apply(lambda x: \"Yes\" if x.isin([\"Yes\"]).sum() == 3 else \"No\", axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "031a2063",
   "metadata": {},
   "source": [
    "loan_no_missing_data_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8c24e7b",
   "metadata": {},
   "source": [
    "# find out were there are outliers\n",
    "loan_no_missing_data_df.ApplicantIncome_is_outlier.value_counts(), loan_no_missing_data_df.CoapplicantIncome_is_outlier.value_counts(), loan_no_missing_data_df.LoanAmount_is_outlier.value_counts(), loan_no_missing_data_df.All_outliers.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31de398b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# focus on rows were all continuous columns are outliers\n",
    "loan_no_missing_data_df[loan_no_missing_data_df[\"All_outliers\"] == \"Yes\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c656e7bd",
   "metadata": {},
   "source": [
    "# Drop the one line where the all continuous features are outlier\n",
    "loan_no_missing_data_df = loan_no_missing_data_df.drop(loan_no_missing_data_df[loan_no_missing_data_df[\"All_outliers\"] == \"Yes\"].index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7513801b",
   "metadata": {},
   "source": [
    "loan_no_missing_data_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f7d05a8",
   "metadata": {},
   "source": [
    "# remove the Loan ID, as it does not give additional information to our data, also remove the new outlier columns add as these are no longer needed\n",
    "loan_no_missing_data_df.drop(columns=[\"Loan_ID\", \"All_outliers\", \"ApplicantIncome_is_outlier\", \"CoapplicantIncome_is_outlier\", \"LoanAmount_is_outlier\"], inplace=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fabf34a",
   "metadata": {},
   "source": [
    "# combine the Applicant and co-applicant income as one to create a new `TotalIncome`\n",
    "loan_no_missing_data_df['TotalIncome'] = loan_no_missing_data_df['ApplicantIncome'] + loan_no_missing_data_df['CoapplicantIncome']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c098b2a",
   "metadata": {},
   "source": [
    "loan_no_missing_data_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34b1a1d6",
   "metadata": {},
   "source": [
    "## Graphical analysis of features that are continuous "
   ]
  },
  {
   "cell_type": "code",
   "id": "1131c437",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0,0].set_title(\"ApplicantIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['ApplicantIncome'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[0,0]);\n",
    "\n",
    "axes[0,1].set_title(\"CoapplicantIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['CoapplicantIncome'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[0,1]);\n",
    "\n",
    "axes[1,0].set_title(\"TotalIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['TotalIncome'], color=\"blue\", kde=True,\n",
    "             stat=\"density\",ax=axes[1,0])\n",
    "\n",
    "axes[1,1].set_title('LoanAmount');\n",
    "sns.histplot(loan_no_missing_data_df['LoanAmount'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[1,1]);\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2608b082",
   "metadata": {},
   "source": [
    "# Create table for just features that are continuous \n",
    "loan_no_missing_data_cf =  loan_no_missing_data_df[['ApplicantIncome','CoapplicantIncome', 'LoanAmount', 'TotalIncome']].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "534bdb76",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "corr = loan_no_missing_data_cf.corr()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87854589",
   "metadata": {},
   "source": [
    "# see the correlation heatmap\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(corr, annot = True, cmap = \"BuPu\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26127b1b",
   "metadata": {},
   "source": [
    "# Drop \"ApplicantIncome\" and \"CoapplicantIncome\" columns now we have a column that sums them together \n",
    "loan_no_missing_data_df.drop(columns=['ApplicantIncome','CoapplicantIncome'], inplace=True)\n",
    "loan_no_missing_data_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95b6ba9a",
   "metadata": {},
   "source": [
    "# save the latest dataframe\n",
    "loan_no_missing_data_df.to_csv(\"loan-no-missing-data.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e82765d",
   "metadata": {},
   "source": [
    "# histogram and density curve to show the distributions for both continuous coloumns(TotalIncome & LoanAmount)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axes[0].set_title(\"TotalIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['TotalIncome'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[0]);\n",
    "\n",
    "axes[1].set_title(\"LoanAmount\")\n",
    "sns.histplot(loan_no_missing_data_df['LoanAmount'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[1]);\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f55cce3",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "4447aaaa",
   "metadata": {},
   "source": [
    "# Apply normalisation to LoanAmount column\n",
    "loan_no_missing_data_df[\"LoanAmount\"] = loan_no_missing_data_df[\"LoanAmount\"].apply(lambda x: (x + loan_no_missing_data_df[\"LoanAmount\"].min())/(loan_no_missing_data_df[\"LoanAmount\"].max() - loan_no_missing_data_df[\"LoanAmount\"].min()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "631b9dcf",
   "metadata": {},
   "source": [
    "# Apply normalisation to TotalIcome column\n",
    "loan_no_missing_data_df[\"TotalIncome\"] = loan_no_missing_data_df[\"TotalIncome\"].apply(lambda x: (x + loan_no_missing_data_df[\"TotalIncome\"].min())/(loan_no_missing_data_df[\"TotalIncome\"].max() - loan_no_missing_data_df[\"TotalIncome\"].min()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "184daa36",
   "metadata": {},
   "source": [
    "# lets see what this two continuous column looks like after transformation\n",
    "loan_no_missing_data_df[[\"LoanAmount\",\"TotalIncome\"]].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c7c8551",
   "metadata": {},
   "source": [
    "# save the latest dataframe\n",
    "loan_no_missing_data_df.to_csv(\"loan-no-missing-data.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8250ed6",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "id": "c847e22b",
   "metadata": {},
   "source": [
    "# Adjust skewness for both continuous coloumns above using logrithm,redraw graph\n",
    "loan_no_missing_data_df['TotalIncome'] = np.log(loan_no_missing_data_df['TotalIncome'])\n",
    "loan_no_missing_data_df['LoanAmount'] = np.log(loan_no_missing_data_df['LoanAmount'])\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axes[0].set_title(\"TotalIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['TotalIncome'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[0]);\n",
    "\n",
    "axes[1].set_title(\"LoanAmount\")\n",
    "sns.histplot(loan_no_missing_data_df['LoanAmount'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[1]);\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c82d626",
   "metadata": {},
   "source": [
    "# LoanAmount is skewed to the left using logarithm transformation, try square root transformation on LoanAmount instead\n",
    "loan_no_missing_data_df = pd.read_csv(\"loan-no-missing-data.csv\")\n",
    "loan_no_missing_data_df['TotalIncome'] = np.log(loan_no_missing_data_df['TotalIncome'])\n",
    "loan_no_missing_data_df['LoanAmount'] = np.sqrt(loan_no_missing_data_df['LoanAmount'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29a16bb6",
   "metadata": {},
   "source": [
    "# plot 'TotalIncome' and 'LoanAmount'\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axes[0].set_title(\"TotalIncome\")\n",
    "sns.histplot(loan_no_missing_data_df['TotalIncome'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[0])\n",
    "\n",
    "axes[1].set_title(\"LoanAmount\")\n",
    "sns.histplot(loan_no_missing_data_df['LoanAmount'], color=\"blue\", kde=True,\n",
    "    stat=\"density\",ax=axes[1]);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8170204",
   "metadata": {},
   "source": [
    "# save the latest dataframe\n",
    "loan_no_missing_data_df.to_csv(\"loan-no-missing-data.csv\", index = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28c23bac",
   "metadata": {},
   "source": [
    "# Get unique values in column \"Dependents\" & \"Loan_Amount_Term\"\n",
    "loan_no_missing_data_df[\"Dependents\"].unique(),loan_no_missing_data_df[\"Loan_Amount_Term\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e70e66ef",
   "metadata": {},
   "source": [
    "# Build my own label encoding\n",
    "class LabelEncoding:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, col):\n",
    "        self.out = {}\n",
    "        for idx, val in enumerate(col.unique()):\n",
    "            self.out[val] = idx\n",
    "            \n",
    "    def transform(self, col):\n",
    "        col = col.apply(lambda x: self.out[x])\n",
    "        return col\n",
    "    \n",
    "    def fit_transform(self, col):\n",
    "        self.fit(col)\n",
    "        return self.transform(col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c837ac5d",
   "metadata": {},
   "source": [
    "# apply label encoding to categorised column ro make this numeric\n",
    "cols = [\"Gender\", \"Dependents\", \"Married\", \"Education\", \"Self_Employed\", \"Property_Area\", \"Loan_Status\", \"Loan_Amount_Term\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cols:\n",
    "    loan_no_missing_data_df[col] = le.fit_transform(loan_no_missing_data_df[col])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8d01ff6",
   "metadata": {},
   "source": [
    "loan_no_missing_data_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5628b61",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957f682",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "id": "649ecf63",
   "metadata": {},
   "source": [
    "# Proceed to build and test our model'\n",
    "# Split features and label dataset, where feature dataset is represented by `X` and label by `y`\n",
    "X = loan_no_missing_data_df.drop(columns=['Loan_Status'], axis=1)\n",
    "y = loan_no_missing_data_df[\"Loan_Status\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb648b17",
   "metadata": {},
   "source": [
    "# Split your train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1000,stratify=y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b40538fc",
   "metadata": {},
   "source": [
    "y.value_counts(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16b24919",
   "metadata": {},
   "source": [
    "y_train.value_counts(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbe6a0ce",
   "metadata": {},
   "source": [
    "# function to fit and score model\n",
    "def classify(model, x_t, y_t):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1000, stratify=y)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"The accuracy: \", model.score(X_test, y_test) * 100)\n",
    "    \n",
    "    score = cross_val_score(model, x_t, y_t, cv=5)\n",
    "    print(\"Cross Validate score: \", np.mean(score) * 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06f51e1c",
   "metadata": {},
   "source": [
    "#### Using logistic regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc2a5991",
   "metadata": {},
   "source": [
    "# Using logistic Regression\n",
    "log_model = LogisticRegression()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9daf431a",
   "metadata": {},
   "source": [
    "# fit / train model\n",
    "log_model.fit(X_train, y_train);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e748c38",
   "metadata": {},
   "source": [
    "# Make a prediction with your test features\n",
    "log_model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad1a6f0f",
   "metadata": {},
   "source": [
    "# actual test label\n",
    "y_test.to_numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe2044c3",
   "metadata": {},
   "source": [
    "# accuracy score for logistic Regression \n",
    "accuracy_score(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65b119cd",
   "metadata": {},
   "source": [
    "# precision score for logistic Regression \n",
    "precision_score(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c0ce1d1",
   "metadata": {},
   "source": [
    "# recall score for logistic Regression \n",
    "recall_score(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc4764d0",
   "metadata": {},
   "source": [
    "score = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Cross Validate score: \", np.mean(score) * 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0041420b",
   "metadata": {},
   "source": [
    "# confusion_matrix for logistic Regression \n",
    "confusion_matrix(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d724f6f2",
   "metadata": {},
   "source": [
    "# produce more colourful confusion matrix\n",
    "def customise_confusion_matrix(y_true, y_pred, normalize=False, title=None, cmap=plt.cm.Purples):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap=cmap)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('Actual labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0de2fb1a",
   "metadata": {},
   "source": [
    "# confusion_matrix for logistic Regression \n",
    "customise_confusion_matrix(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b04a8905",
   "metadata": {},
   "source": [
    "# OR\n",
    "import sklearn\n",
    "\n",
    "# Note this is from a fitted model and not predictions. Also, note we are passing all X and y date, \n",
    "# hence why we are getting more data\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=log_model,X=X, y=y);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "adde2f7d",
   "metadata": {},
   "source": [
    "# Note this is from a predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test,y_pred=log_model.predict(X_test));"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85a3bcb8",
   "metadata": {},
   "source": [
    "# confusion_matrix for logistic Regression \n",
    "roc_auc_score(y_test, log_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d764c5e4",
   "metadata": {},
   "source": [
    "#### Using RandomForestClassifier algorithm"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3243209",
   "metadata": {},
   "source": [
    "# RandomForestClassifier algorithm\n",
    "# Try different amount of n_estimate to see which is better\n",
    "np.random.seed(1000) # so our results are replicable\n",
    "for i in range (10, 200, 10):\n",
    "    print(f\"Trying model with {i} estimators ...\")\n",
    "    rand_model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    print(f\"Model accuracy on the test set: {accuracy_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")\n",
    "    print(f\"Model precision on the test set: {precision_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")\n",
    "    print(f\"Model recall on the test set: {recall_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cd3e29a",
   "metadata": {},
   "source": [
    "**The results above indicate that the best estimator for the chosen range is `90`. For this model training `n_estimator = 90` will be used**"
   ]
  },
  {
   "cell_type": "code",
   "id": "cef42105",
   "metadata": {},
   "source": [
    "np.random.seed(1000) # so our results are replicable\n",
    "rand_model = RandomForestClassifier(n_estimators=130).fit(X_train, y_train)\n",
    "print(f\"Model accuracy on the test set: {accuracy_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")\n",
    "print(f\"Model precision on the test set: {precision_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")\n",
    "print(f\"Model recall on the test set: {recall_score(y_test, rand_model.predict(X_test)) * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fe692e3",
   "metadata": {},
   "source": [
    "# confusion_matrix for RandomForestClassifier \n",
    "confusion_matrix(y_test, rand_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c99fcff4",
   "metadata": {},
   "source": [
    "# confusion_matrix for RandomForestClassifier\n",
    "customise_confusion_matrix(y_test, rand_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a45b9a53",
   "metadata": {},
   "source": [
    "score = cross_val_score(rand_model, X, y, cv=5)\n",
    "print(\"Cross Validate score: \", np.mean(score) * 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7aa83c47",
   "metadata": {},
   "source": [
    "# confusion_matrix for RandomForestClassifier \n",
    "roc_auc_score(y_test, rand_model.predict(X_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0fbb169c",
   "metadata": {},
   "source": [
    "**Area under the receiver operating characteristics curve (AUC/POC) using results for RFC model**\n",
    "\n",
    "* Area under curve (AUC)\n",
    "* ROC curve\n",
    "\n",
    "ROC curve: comparisons a model's true positive rate (`tpr`) versus a models false positive rate (`fpr`)\n",
    "\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "320ca66f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Make predictions with probabilities\n",
    "y_probs = rand_model.predict_proba(X_test)\n",
    "y_probs[:10], len(y_probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95cf6b0f",
   "metadata": {},
   "source": [
    "y_probs_positive = y_probs[:,1]\n",
    "y_probs_positive[:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36ac33f1",
   "metadata": {},
   "source": [
    "# Calculate fpr, tpr and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "fpr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a314409a",
   "metadata": {},
   "source": [
    "# Create a function for plotting ROC cureve\n",
    "\n",
    "def roc_curve(fpr, tpr):\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "    # Plot line with no predictive power (baselne)\n",
    "    plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Random guess\")\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel(\"False positive rate (fpr)\")\n",
    "    plt.ylabel(\"True positive rate (tpr)\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "roc_curve(fpr, tpr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b699c33f",
   "metadata": {},
   "source": [
    "# Using the logistics regression print classification report\n",
    "print(classification_report(y_test,log_model.predict(X_test)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c80bdad0",
   "metadata": {},
   "source": [
    "pd.DataFrame(classification_report(y_test,log_model.predict(X_test), output_dict=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0a42eca",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
